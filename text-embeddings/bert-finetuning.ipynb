{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13351970,"sourceType":"datasetVersion","datasetId":8448140}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset, Dataset\nfrom trl import SFTTrainer, setup_chat_format\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:12:02.464903Z","iopub.execute_input":"2025-10-13T13:12:02.465200Z","iopub.status.idle":"2025-10-13T13:12:27.499530Z","shell.execute_reply.started":"2025-10-13T13:12:02.465169Z","shell.execute_reply":"2025-10-13T13:12:27.498950Z"}},"outputs":[{"name":"stderr","text":"2025-10-13 13:12:12.229152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760361132.409409     130 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760361132.461146     130 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:16:24.486687Z","iopub.execute_input":"2025-10-13T13:16:24.487329Z","iopub.status.idle":"2025-10-13T13:16:24.673643Z","shell.execute_reply.started":"2025-10-13T13:16:24.487303Z","shell.execute_reply":"2025-10-13T13:16:24.673093Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='fine-tuning bert', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:16:27.208192Z","iopub.execute_input":"2025-10-13T13:16:27.208945Z","iopub.status.idle":"2025-10-13T13:16:39.416163Z","shell.execute_reply.started":"2025-10-13T13:16:27.208917Z","shell.execute_reply":"2025-10-13T13:16:39.415575Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgargibendale\u001b[0m (\u001b[33mgargibendale-university-of-mumbai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.22.2"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251013_131633-dh8xjqpr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert/runs/dh8xjqpr?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">jolly-frost-2</a></strong> to <a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert/runs/dh8xjqpr?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert/runs/dh8xjqpr?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Do NOT share these links with anyone. They can be used to claim your runs."},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model = \"llama-3.2-3b-it-Ecommerce-ChatBot\"\ndataset_name = \"bitext/Bitext-customer-support-llm-chatbot-training-dataset\"","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:16:46.694235Z","iopub.execute_input":"2025-10-13T13:16:46.694521Z","iopub.status.idle":"2025-10-13T13:16:46.700755Z","shell.execute_reply.started":"2025-10-13T13:16:46.694498Z","shell.execute_reply":"2025-10-13T13:16:46.700154Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:19.680488Z","iopub.execute_input":"2025-10-13T13:39:19.680757Z","iopub.status.idle":"2025-10-13T13:39:19.688181Z","shell.execute_reply.started":"2025-10-13T13:39:19.680738Z","shell.execute_reply":"2025-10-13T13:39:19.687463Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    quantization_config=bnb_config,\n    attn_implementation=attn_implementation,\n    num_labels=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:19.858561Z","iopub.execute_input":"2025-10-13T13:39:19.859150Z","iopub.status.idle":"2025-10-13T13:39:20.504159Z","shell.execute_reply.started":"2025-10-13T13:39:19.859097Z","shell.execute_reply":"2025-10-13T13:39:20.503404Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# 3. Prepare for k-bit LoRA training\nmodel = prepare_model_for_kbit_training(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:22.838372Z","iopub.execute_input":"2025-10-13T13:39:22.838641Z","iopub.status.idle":"2025-10-13T13:39:22.850420Z","shell.execute_reply.started":"2025-10-13T13:39:22.838622Z","shell.execute_reply":"2025-10-13T13:39:22.849679Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:41.130461Z","iopub.execute_input":"2025-10-13T13:18:41.130726Z","iopub.status.idle":"2025-10-13T13:18:42.474359Z","shell.execute_reply.started":"2025-10-13T13:18:41.130707Z","shell.execute_reply":"2025-10-13T13:18:42.473579Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d4f8d9eb38a43eda35b0569dadf9125"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88babdf4897f45d99fbb95e44d877fa5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee181c8636ad4464b970a99b6563f2d9"}},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/my-dataset/final_train_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:44.671102Z","iopub.execute_input":"2025-10-13T13:18:44.671460Z","iopub.status.idle":"2025-10-13T13:18:47.480845Z","shell.execute_reply.started":"2025-10-13T13:18:44.671429Z","shell.execute_reply":"2025-10-13T13:18:47.480057Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import ast\n\ndf[\"bullet_points\"] = df[\"bullet_points\"].apply(\n    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n)\n# Merge the list into a single string\ndf[\"bullet_points\"] = df[\"bullet_points\"].apply(\n    lambda x: \" \".join(x) if isinstance(x, list) else x\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:50.333049Z","iopub.execute_input":"2025-10-13T13:18:50.333354Z","iopub.status.idle":"2025-10-13T13:18:51.237383Z","shell.execute_reply.started":"2025-10-13T13:18:50.333331Z","shell.execute_reply":"2025-10-13T13:18:51.236751Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"df[\"text\"] = (df[\"item_name\"].fillna(\"\") + \" \" +\n    df[\"bullet_points\"].fillna(\"\") + \" \" +\n    \"value: \" + df[\"value\"].fillna(\"\").astype(str) + \" \" +\n    \"unit: \" + df[\"unit\"].fillna(\"\")\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:52.803649Z","iopub.execute_input":"2025-10-13T13:18:52.803936Z","iopub.status.idle":"2025-10-13T13:18:52.981642Z","shell.execute_reply.started":"2025-10-13T13:18:52.803914Z","shell.execute_reply":"2025-10-13T13:18:52.981030Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Create a new DataFrame with selected columns\ndf_train = df[['text', 'price']].copy()\ndf_train = df_train.rename(columns={\"price\": \"label\"})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:18:56.921441Z","iopub.execute_input":"2025-10-13T13:18:56.922028Z","iopub.status.idle":"2025-10-13T13:18:56.938260Z","shell.execute_reply.started":"2025-10-13T13:18:56.922002Z","shell.execute_reply":"2025-10-13T13:18:56.937429Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"y = df.price.values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:19:00.065209Z","iopub.execute_input":"2025-10-13T13:19:00.065495Z","iopub.status.idle":"2025-10-13T13:19:00.070845Z","shell.execute_reply.started":"2025-10-13T13:19:00.065474Z","shell.execute_reply":"2025-10-13T13:19:00.070170Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"input_ids = []\nattention_masks = []\nfor sentence in tqdm(df_train[\"text\"]):\n    encoded_dict = tokenizer.encode_plus(\n                        sentence,                      # Sentence to encode.\n                        truncation=True,\n                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n                        max_length = 512,           # Pad & truncate all sentences.\n                        padding=\"max_length\",\n                        return_attention_mask = True,   # Construct attn. masks.\n                        return_tensors = 'pt',     # Return pytorch tensors.\n                   )\n    # Add the encoded sentence to the list.    \n    input_ids.append(encoded_dict['input_ids'])\n    # And its attention mask (simply differentiates padding from non-padding).\n    attention_masks.append(encoded_dict['attention_mask'])\n# Convert the lists into tensors.\ninput_ids = torch.cat(input_ids, dim=0)\nattention_masks = torch.cat(attention_masks, dim=0)\ny = torch.tensor(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:35:52.031909Z","iopub.execute_input":"2025-10-13T13:35:52.032575Z","iopub.status.idle":"2025-10-13T13:36:49.259128Z","shell.execute_reply.started":"2025-10-13T13:35:52.032539Z","shell.execute_reply":"2025-10-13T13:36:49.258206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/56102 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681d3fb861d04ff39d703a649c71027c"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_130/398237897.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  y = torch.tensor(y)\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"type(input_ids)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_inputs, val_inputs, train_masks, val_masks, train_labels, val_labels = train_test_split(\n    input_ids, attention_masks, y, test_size=0.2, random_state=42\n)\n\ntrain_dataset = Dataset.from_dict({\n    \"input_ids\": train_inputs,\n    \"attention_mask\": train_masks,\n    \"labels\": train_labels\n})\n\neval_dataset = Dataset.from_dict({\n    \"input_ids\": val_inputs,\n    \"attention_mask\": val_masks,\n    \"labels\": val_labels\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:02.047595Z","iopub.execute_input":"2025-10-13T13:39:02.047888Z","iopub.status.idle":"2025-10-13T13:39:05.246733Z","shell.execute_reply.started":"2025-10-13T13:39:02.047866Z","shell.execute_reply":"2025-10-13T13:39:05.245956Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"#Importing the dataset\ndataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\n# Create a 90/10 train-test split\ndataset = dataset.train_test_split(test_size=0.1)\ninstruction = \"\"\"You are a top-rated customer service agent named John. \n    Be polite to customers and answer all their questions.\n    \"\"\"\ndef format_chat_template(row):\n    \n    row_json = [{\"role\": \"system\", \"content\": instruction },\n               {\"role\": \"user\", \"content\": row[\"instruction\"]},\n               {\"role\": \"assistant\", \"content\": row[\"response\"]}]\n    \n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset['train']['text'][3]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"modules","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16, #rank of the lora matrix\n    lora_alpha=32,#learning rate\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"SEQ_CLS\",\n    target_modules=[\"query\", \"value\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:38.213736Z","iopub.execute_input":"2025-10-13T13:39:38.214055Z","iopub.status.idle":"2025-10-13T13:39:38.261811Z","shell.execute_reply.started":"2025-10-13T13:39:38.214031Z","shell.execute_reply":"2025-10-13T13:39:38.261277Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=\"/kaggle/working/new-model\",\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    logging_steps=100,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=1e-4,\n    group_by_length=True,\n    report_to=\"wandb\",\n    fp16=True,\n    bf16=False,\n    dataloader_num_workers=4,  # Add to TrainingArguments\ndataloader_pin_memory=True,\n    gradient_checkpointing=True,  # Add to TrainingArguments\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:41.880665Z","iopub.execute_input":"2025-10-13T13:39:41.881399Z","iopub.status.idle":"2025-10-13T13:39:41.910519Z","shell.execute_reply.started":"2025-10-13T13:39:41.881374Z","shell.execute_reply":"2025-10-13T13:39:41.909810Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"from sklearn.metrics import mean_absolute_error, mean_squared_error\n\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.squeeze()\n    \n    # SMAPE\n    numerator = np.abs(predictions - labels)\n    denominator = (np.abs(predictions) + np.abs(labels)) / 2\n    mask = denominator != 0\n    smape = np.where(mask, numerator[mask] / denominator[mask], 0)\n    smape = np.mean(smape) * 100\n    \n    # Other metrics\n    mae = mean_absolute_error(labels, predictions)\n    rmse = np.sqrt(mean_squared_error(labels, predictions))\n    \n    return {\n        \"smape\": smape,\n        \"mae\": mae,\n        \"rmse\": rmse\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:29:05.435944Z","iopub.execute_input":"2025-10-13T13:29:05.436244Z","iopub.status.idle":"2025-10-13T13:29:05.442805Z","shell.execute_reply.started":"2025-10-13T13:29:05.436219Z","shell.execute_reply":"2025-10-13T13:29:05.442187Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = Trainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    args=training_arguments,\n    compute_metrics=compute_metrics,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:39:50.054833Z","iopub.execute_input":"2025-10-13T13:39:50.055358Z","iopub.status.idle":"2025-10-13T13:39:50.069436Z","shell.execute_reply.started":"2025-10-13T13:39:50.055336Z","shell.execute_reply":"2025-10-13T13:39:50.068650Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T13:40:00.894819Z","iopub.execute_input":"2025-10-13T13:40:00.895098Z","iopub.status.idle":"2025-10-13T14:37:28.727848Z","shell.execute_reply.started":"2025-10-13T13:40:00.895079Z","shell.execute_reply":"2025-10-13T14:37:28.727024Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1403' max='1403' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1403/1403 57:07, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Smape</th>\n      <th>Mae</th>\n      <th>Rmse</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>328.604200</td>\n      <td>312.954254</td>\n      <td>63.363272</td>\n      <td>11.767222</td>\n      <td>17.690512</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>292.201400</td>\n      <td>281.591339</td>\n      <td>62.098002</td>\n      <td>11.586773</td>\n      <td>16.780684</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1403, training_loss=329.0363700360973, metrics={'train_runtime': 3429.8185, 'train_samples_per_second': 13.086, 'train_steps_per_second': 0.409, 'total_flos': 1.1890008924592128e+16, 'train_loss': 329.0363700360973, 'epoch': 1.0})"},"metadata":{}}],"execution_count":48},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:41:07.912605Z","iopub.execute_input":"2025-10-13T14:41:07.912942Z","iopub.status.idle":"2025-10-13T14:41:08.381288Z","shell.execute_reply.started":"2025-10-13T14:41:07.912907Z","shell.execute_reply":"2025-10-13T14:41:08.380556Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/mae</td><td>█▁</td></tr><tr><td>eval/rmse</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/smape</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▃▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/global_step</td><td>▁▂▂▃▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train/grad_norm</td><td>▃▁▂▁▃▁▂▂█▅▃▂▃▃▂▂▄▂</td></tr><tr><td>+2</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>281.59134</td></tr><tr><td>eval/mae</td><td>11.58677</td></tr><tr><td>eval/rmse</td><td>16.78068</td></tr><tr><td>eval/runtime</td><td>226.5578</td></tr><tr><td>eval/samples_per_second</td><td>49.528</td></tr><tr><td>eval/smape</td><td>62.098</td></tr><tr><td>eval/steps_per_second</td><td>3.099</td></tr><tr><td>total_flos</td><td>1.1890008924592128e+16</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>1403</td></tr><tr><td>+7</td><td>...</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jolly-frost-2</strong> at: <a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert/runs/dh8xjqpr?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert/runs/dh8xjqpr?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9</a><br> View project at: <a href='https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9' target=\"_blank\">https://wandb.ai/gargibendale-university-of-mumbai/fine-tuning%20bert?apiKey=8b649ca0bcc74b1937c6bb95966027b67dd25ea9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251013_131633-dh8xjqpr/logs</code>"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"messages = [{\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"I bought the same item twice, cancel order {{Order Number}}\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save the fine-tuned model\ntrainer.save_model(\"/kaggle/working/final-model-large\")\ntokenizer.save_pretrained(\"/kaggle/working/final-model-large\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T14:42:32.660833Z","iopub.execute_input":"2025-10-13T14:42:32.661478Z","iopub.status.idle":"2025-10-13T14:42:32.952196Z","shell.execute_reply.started":"2025-10-13T14:42:32.661454Z","shell.execute_reply":"2025-10-13T14:42:32.951416Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"('/kaggle/working/final-model-large/tokenizer_config.json',\n '/kaggle/working/final-model-large/special_tokens_map.json',\n '/kaggle/working/final-model-large/vocab.txt',\n '/kaggle/working/final-model-large/added_tokens.json',\n '/kaggle/working/final-model-large/tokenizer.json')"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}