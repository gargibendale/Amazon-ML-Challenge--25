{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13351970,"sourceType":"datasetVersion","datasetId":8448140},{"sourceId":13370750,"sourceType":"datasetVersion","datasetId":8482148}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    Trainer,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset, Dataset\nfrom trl import SFTTrainer, setup_chat_format\nimport numpy as np\nimport pandas as pd\nimport transformers\nfrom tqdm.auto import tqdm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:21:08.244626Z","iopub.execute_input":"2025-10-13T15:21:08.245360Z","iopub.status.idle":"2025-10-13T15:21:12.183146Z","shell.execute_reply.started":"2025-10-13T15:21:08.245337Z","shell.execute_reply":"2025-10-13T15:21:12.182504Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    \"bert-base-uncased\",\n    quantization_config=bnb_config,\n    attn_implementation=\"eager\",\n    num_labels=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:21:15.760793Z","iopub.execute_input":"2025-10-13T15:21:15.761044Z","iopub.status.idle":"2025-10-13T15:21:16.513647Z","shell.execute_reply.started":"2025-10-13T15:21:15.761028Z","shell.execute_reply":"2025-10-13T15:21:16.513033Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# Load LoRA weights on top\nmodel = PeftModel.from_pretrained(base_model, \"/kaggle/input/bert-fine-tuned-lora/bert-ft-normal/bert-ft-normal\").eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:21:59.988942Z","iopub.execute_input":"2025-10-13T15:21:59.989681Z","iopub.status.idle":"2025-10-13T15:22:00.090587Z","shell.execute_reply.started":"2025-10-13T15:21:59.989655Z","shell.execute_reply":"2025-10-13T15:22:00.089979Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:22:21.113747Z","iopub.execute_input":"2025-10-13T15:22:21.114392Z","iopub.status.idle":"2025-10-13T15:22:21.129835Z","shell.execute_reply.started":"2025-10-13T15:22:21.114362Z","shell.execute_reply":"2025-10-13T15:22:21.129157Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"PeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): BertForSequenceClassification(\n      (bert): BertModel(\n        (embeddings): BertEmbeddings(\n          (word_embeddings): Embedding(30522, 768, padding_idx=0)\n          (position_embeddings): Embedding(512, 768)\n          (token_type_embeddings): Embedding(2, 768)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (encoder): BertEncoder(\n          (layer): ModuleList(\n            (0-11): 12 x BertLayer(\n              (attention): BertAttention(\n                (self): BertSelfAttention(\n                  (query): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (key): Linear4bit(in_features=768, out_features=768, bias=True)\n                  (value): lora.Linear4bit(\n                    (base_layer): Linear4bit(in_features=768, out_features=768, bias=True)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.05, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=768, out_features=16, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=16, out_features=768, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                    (lora_magnitude_vector): ModuleDict()\n                  )\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n                (output): BertSelfOutput(\n                  (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                )\n              )\n              (intermediate): BertIntermediate(\n                (dense): Linear4bit(in_features=768, out_features=3072, bias=True)\n                (intermediate_act_fn): GELUActivation()\n              )\n              (output): BertOutput(\n                (dense): Linear4bit(in_features=3072, out_features=768, bias=True)\n                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (pooler): BertPooler(\n          (dense): Linear4bit(in_features=768, out_features=768, bias=True)\n          (activation): Tanh()\n        )\n      )\n      (dropout): Dropout(p=0.1, inplace=False)\n      (classifier): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=768, out_features=1, bias=True)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=768, out_features=1, bias=True)\n        )\n      )\n    )\n  )\n)"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/bert-fine-tuned-lora/bert-ft-normal/bert-ft-normal\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:00.531631Z","iopub.execute_input":"2025-10-13T15:23:00.532349Z","iopub.status.idle":"2025-10-13T15:23:00.638536Z","shell.execute_reply.started":"2025-10-13T15:23:00.532324Z","shell.execute_reply":"2025-10-13T15:23:00.637781Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from torch.utils.data import DataLoader, TensorDataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:20.645220Z","iopub.execute_input":"2025-10-13T15:23:20.645926Z","iopub.status.idle":"2025-10-13T15:23:20.649205Z","shell.execute_reply.started":"2025-10-13T15:23:20.645902Z","shell.execute_reply":"2025-10-13T15:23:20.648462Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def get_embeddings(texts, model, tokenizer, batch_size=32):\n    \"\"\"\n    Generate embeddings for a list of texts.\n    Returns the [CLS] token embeddings from BERT's last hidden layer.\n    \"\"\"\n    embeddings_list = []\n    \n    # Process in batches\n    for i in tqdm(range(0, len(texts), batch_size)):\n        batch_texts = texts[i:i + batch_size]\n        \n        # Tokenize\n        encoded = tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            max_length=256,\n            return_tensors='pt'\n        )\n        \n        # Move to GPU if available\n        input_ids = encoded['input_ids'].to(model.device)\n        attention_mask = encoded['attention_mask'].to(model.device)\n        \n        # Get embeddings (no gradient calculation needed)\n        with torch.no_grad():\n            outputs = model.base_model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=True\n            )\n            \n            # Get [CLS] token embedding from last hidden layer\n            # Shape: (batch_size, hidden_size)\n            cls_embeddings = outputs.hidden_states[-1][:, 0, :]\n            \n        embeddings_list.append(cls_embeddings.cpu())\n    \n    # Concatenate all batches\n    return torch.cat(embeddings_list, dim=0).numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:28.075312Z","iopub.execute_input":"2025-10-13T15:23:28.075893Z","iopub.status.idle":"2025-10-13T15:23:28.081290Z","shell.execute_reply.started":"2025-10-13T15:23:28.075869Z","shell.execute_reply":"2025-10-13T15:23:28.080663Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/my-dataset/final_train_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:28.811457Z","iopub.execute_input":"2025-10-13T15:23:28.812109Z","iopub.status.idle":"2025-10-13T15:23:30.370499Z","shell.execute_reply.started":"2025-10-13T15:23:28.812085Z","shell.execute_reply":"2025-10-13T15:23:30.369657Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import ast\n\ndf[\"bullet_points\"] = df[\"bullet_points\"].apply(\n    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n)\n# Merge the list into a single string\ndf[\"bullet_points\"] = df[\"bullet_points\"].apply(\n    lambda x: \" \".join(x) if isinstance(x, list) else x\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:30.617147Z","iopub.execute_input":"2025-10-13T15:23:30.617769Z","iopub.status.idle":"2025-10-13T15:23:31.480763Z","shell.execute_reply.started":"2025-10-13T15:23:30.617743Z","shell.execute_reply":"2025-10-13T15:23:31.480159Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"df[\"text\"] = (df[\"item_name\"].fillna(\"\") + \" \" +\n    df[\"bullet_points\"].fillna(\"\") + \" \" +\n    \"value: \" + df[\"value\"].fillna(\"\").astype(str) + \" \" +\n    \"unit: \" + df[\"unit\"].fillna(\"\")\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:31.481895Z","iopub.execute_input":"2025-10-13T15:23:31.482102Z","iopub.status.idle":"2025-10-13T15:23:31.658744Z","shell.execute_reply.started":"2025-10-13T15:23:31.482087Z","shell.execute_reply":"2025-10-13T15:23:31.657989Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"# Generate embeddings for train set\ntrain_texts = df['text'].tolist()\ntrain_embeddings = get_embeddings(train_texts, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:23:37.302516Z","iopub.execute_input":"2025-10-13T15:23:37.303076Z","iopub.status.idle":"2025-10-13T15:31:06.566105Z","shell.execute_reply.started":"2025-10-13T15:23:37.303052Z","shell.execute_reply":"2025-10-13T15:31:06.565488Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1754 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c559343355ca46abac3bc023e34d0ea4"}},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"train_embeddings.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:31:16.152725Z","iopub.execute_input":"2025-10-13T15:31:16.153542Z","iopub.status.idle":"2025-10-13T15:31:16.158250Z","shell.execute_reply.started":"2025-10-13T15:31:16.153517Z","shell.execute_reply":"2025-10-13T15:31:16.157635Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"(56102, 768)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"type(train_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:31:26.499024Z","iopub.execute_input":"2025-10-13T15:31:26.499744Z","iopub.status.idle":"2025-10-13T15:31:26.504469Z","shell.execute_reply.started":"2025-10-13T15:31:26.499722Z","shell.execute_reply":"2025-10-13T15:31:26.503829Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"numpy.ndarray"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"np.save('/kaggle/working/finetuned-bert-train.npy', train_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:32:40.130809Z","iopub.execute_input":"2025-10-13T15:32:40.131089Z","iopub.status.idle":"2025-10-13T15:32:40.191757Z","shell.execute_reply.started":"2025-10-13T15:32:40.131069Z","shell.execute_reply":"2025-10-13T15:32:40.191164Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"dftest = pd.read_csv('/kaggle/input/my-dataset/final_test_dataset.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:32:57.282901Z","iopub.execute_input":"2025-10-13T15:32:57.283397Z","iopub.status.idle":"2025-10-13T15:33:00.349536Z","shell.execute_reply.started":"2025-10-13T15:32:57.283374Z","shell.execute_reply":"2025-10-13T15:33:00.348882Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"import ast\n\ndftest[\"bullet_points\"] = dftest[\"bullet_points\"].apply(\n    lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n)\n# Merge the list into a single string\ndftest[\"bullet_points\"] = dftest[\"bullet_points\"].apply(\n    lambda x: \" \".join(x) if isinstance(x, list) else x\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:00.350803Z","iopub.execute_input":"2025-10-13T15:33:00.351312Z","iopub.status.idle":"2025-10-13T15:33:01.821195Z","shell.execute_reply.started":"2025-10-13T15:33:00.351282Z","shell.execute_reply":"2025-10-13T15:33:01.820383Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"dftest[\"text\"] = (dftest[\"item_name\"].fillna(\"\") + \" \" +\n    dftest[\"bullet_points\"].fillna(\"\") + \" \" +\n    \"value: \" + dftest[\"value\"].fillna(\"\").astype(str) + \" \" +\n    \"unit: \" + dftest[\"unit\"].fillna(\"\")\n                )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:01.822515Z","iopub.execute_input":"2025-10-13T15:33:01.822720Z","iopub.status.idle":"2025-10-13T15:33:02.035587Z","shell.execute_reply.started":"2025-10-13T15:33:01.822704Z","shell.execute_reply":"2025-10-13T15:33:02.034982Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"dftest.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:09.428323Z","iopub.execute_input":"2025-10-13T15:33:09.428856Z","iopub.status.idle":"2025-10-13T15:33:09.448874Z","shell.execute_reply.started":"2025-10-13T15:33:09.428835Z","shell.execute_reply":"2025-10-13T15:33:09.448165Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"   sample_id                                    catalog_content  \\\n0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n2     146263  Item Name: Honey Filled Hard Candy - Bulk Pack...   \n3      95658  Item Name: Vlasic Snack'mm's Kosher Dill 16 Ou...   \n4      36806  Item Name: McCormick Culinary Vanilla Extract,...   \n\n                                          image_link  \\\n0  https://m.media-amazon.com/images/I/71hoAn78AW...   \n1  https://m.media-amazon.com/images/I/61ex8NHCIj...   \n2  https://m.media-amazon.com/images/I/61KCM61J8e...   \n3  https://m.media-amazon.com/images/I/51Ex6uOH7y...   \n4  https://m.media-amazon.com/images/I/71QYlrOMoS...   \n\n                                           item_name brand_name  \\\n0  Rani 14-Spice Eshamaya's Mango Chutney (Indian...       Rani   \n1  Natural MILK TEA Flavoring extract by HALO PAN...    Natural   \n2  Honey Filled Hard Candy - Bulk Pack 2 Pounds -...      Honey   \n3  Vlasic Snack'mm's Kosher Dill 16 Ounce (Pack o...     Vlasic   \n4  McCormick Culinary Vanilla Extract, 32 Fluid_O...  McCormick   \n\n                                       bullet_points  \\\n0  youll love 14spice eshamayas mango chutney chu...   \n1  authentic tasting asianinspired natural flavor...   \n2  honey filled hard candy 2pound bulk pack appro...   \n3                                                      \n4  premium ingredients mccormick culinary pure va...   \n\n                                 product_description  value         unit  \\\n0  mango chutney made diced green mangoes cooked ...   10.5        Ounce   \n1  check popular milk tea flavoring extract new l...    2.0  Fluid_Ounce   \n2  honey filled hard candy bulk pack pounds indiv...   32.0        Ounce   \n3                                                NaN    2.0        Count   \n4                                                NaN   32.0  Fluid_Ounce   \n\n                                                text  \n0  Rani 14-Spice Eshamaya's Mango Chutney (Indian...  \n1  Natural MILK TEA Flavoring extract by HALO PAN...  \n2  Honey Filled Hard Candy - Bulk Pack 2 Pounds -...  \n3  Vlasic Snack'mm's Kosher Dill 16 Ounce (Pack o...  \n4  McCormick Culinary Vanilla Extract, 32 Fluid_O...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample_id</th>\n      <th>catalog_content</th>\n      <th>image_link</th>\n      <th>item_name</th>\n      <th>brand_name</th>\n      <th>bullet_points</th>\n      <th>product_description</th>\n      <th>value</th>\n      <th>unit</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100179</td>\n      <td>Item Name: Rani 14-Spice Eshamaya's Mango Chut...</td>\n      <td>https://m.media-amazon.com/images/I/71hoAn78AW...</td>\n      <td>Rani 14-Spice Eshamaya's Mango Chutney (Indian...</td>\n      <td>Rani</td>\n      <td>youll love 14spice eshamayas mango chutney chu...</td>\n      <td>mango chutney made diced green mangoes cooked ...</td>\n      <td>10.5</td>\n      <td>Ounce</td>\n      <td>Rani 14-Spice Eshamaya's Mango Chutney (Indian...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>245611</td>\n      <td>Item Name: Natural MILK TEA Flavoring extract ...</td>\n      <td>https://m.media-amazon.com/images/I/61ex8NHCIj...</td>\n      <td>Natural MILK TEA Flavoring extract by HALO PAN...</td>\n      <td>Natural</td>\n      <td>authentic tasting asianinspired natural flavor...</td>\n      <td>check popular milk tea flavoring extract new l...</td>\n      <td>2.0</td>\n      <td>Fluid_Ounce</td>\n      <td>Natural MILK TEA Flavoring extract by HALO PAN...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>146263</td>\n      <td>Item Name: Honey Filled Hard Candy - Bulk Pack...</td>\n      <td>https://m.media-amazon.com/images/I/61KCM61J8e...</td>\n      <td>Honey Filled Hard Candy - Bulk Pack 2 Pounds -...</td>\n      <td>Honey</td>\n      <td>honey filled hard candy 2pound bulk pack appro...</td>\n      <td>honey filled hard candy bulk pack pounds indiv...</td>\n      <td>32.0</td>\n      <td>Ounce</td>\n      <td>Honey Filled Hard Candy - Bulk Pack 2 Pounds -...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>95658</td>\n      <td>Item Name: Vlasic Snack'mm's Kosher Dill 16 Ou...</td>\n      <td>https://m.media-amazon.com/images/I/51Ex6uOH7y...</td>\n      <td>Vlasic Snack'mm's Kosher Dill 16 Ounce (Pack o...</td>\n      <td>Vlasic</td>\n      <td></td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>Count</td>\n      <td>Vlasic Snack'mm's Kosher Dill 16 Ounce (Pack o...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>36806</td>\n      <td>Item Name: McCormick Culinary Vanilla Extract,...</td>\n      <td>https://m.media-amazon.com/images/I/71QYlrOMoS...</td>\n      <td>McCormick Culinary Vanilla Extract, 32 Fluid_O...</td>\n      <td>McCormick</td>\n      <td>premium ingredients mccormick culinary pure va...</td>\n      <td>NaN</td>\n      <td>32.0</td>\n      <td>Fluid_Ounce</td>\n      <td>McCormick Culinary Vanilla Extract, 32 Fluid_O...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"type(dftest['text'][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:27.546378Z","iopub.execute_input":"2025-10-13T15:33:27.546641Z","iopub.status.idle":"2025-10-13T15:33:27.551548Z","shell.execute_reply.started":"2025-10-13T15:33:27.546623Z","shell.execute_reply":"2025-10-13T15:33:27.550963Z"}},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"str"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"# Generate embeddings for test set\ntest_texts = dftest['text'].tolist()  # Assuming you have df_test\ntest_embeddings = get_embeddings(test_texts, model, tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:33:42.463508Z","iopub.execute_input":"2025-10-13T15:33:42.463764Z","iopub.status.idle":"2025-10-13T15:43:34.589778Z","shell.execute_reply.started":"2025-10-13T15:33:42.463745Z","shell.execute_reply":"2025-10-13T15:43:34.588905Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/2344 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eabf77a925414266a0902541db204e94"}},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"np.save('/kaggle/working/finetuned-bert-test.npy', test_embeddings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T15:43:35.805357Z","iopub.execute_input":"2025-10-13T15:43:35.805916Z","iopub.status.idle":"2025-10-13T15:43:35.891094Z","shell.execute_reply.started":"2025-10-13T15:43:35.805891Z","shell.execute_reply":"2025-10-13T15:43:35.890491Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}