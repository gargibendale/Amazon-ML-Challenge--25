{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.ensemble import StackingRegressor, VotingRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import QuantileTransformer, LabelEncoder\nimport joblib\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# GPU setup\nimport tensorflow as tf\nif tf.config.list_physical_devices('GPU'):\n    tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')\n    print(\"GPU enabled\")\nelse:\n    print(\"No GPU found\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:44:19.124009Z","iopub.execute_input":"2025-10-12T21:44:19.124315Z","iopub.status.idle":"2025-10-12T21:44:26.964782Z","shell.execute_reply.started":"2025-10-12T21:44:19.124294Z","shell.execute_reply":"2025-10-12T21:44:26.96399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gdown\n\nTRAIN_FILE_ID = '1BwXlIE1W2DSiar68_mofb1SRc1hO8OA7'\nTEST_FILE_ID = '1UMh9L32RG0JeKTjgH6A_-0x7bbTugWWS'\n\nprint(\"Downloading datasets from Google Drive...\")\n\n# Train dataset\ntrain_url = f'https://drive.google.com/uc?id={TRAIN_FILE_ID}'\ngdown.download(train_url, 'train_dataset.csv', quiet=False)\n\n# Test dataset\ntest_url = f'https://drive.google.com/uc?id={TEST_FILE_ID}'\ngdown.download(test_url, 'test_dataset.csv', quiet=False)\n\nprint(\"Download completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:44:26.966209Z","iopub.execute_input":"2025-10-12T21:44:26.966894Z","iopub.status.idle":"2025-10-12T21:45:11.35873Z","shell.execute_reply.started":"2025-10-12T21:44:26.966871Z","shell.execute_reply":"2025-10-12T21:45:11.35785Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_train = pd.read_csv(\"train_dataset.csv\")\ndf_test = pd.read_csv(\"test_dataset.csv\")\n\nprint(f\"Train shape: {df_train.shape}\")\nprint(f\"Test shape: {df_test.shape}\")\n\n# Check target distribution\nprint(\"\\nTarget (price) statistics:\")\nprint(df_train['price'].describe())\n\n# Check for missing values\nprint(\"\\nMissing values in train:\")\nprint(df_train.isnull().sum())\nprint(\"\\nMissing values in test:\")\nprint(df_test.isnull().sum())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:45:11.359542Z","iopub.execute_input":"2025-10-12T21:45:11.36027Z","iopub.status.idle":"2025-10-12T21:46:01.820613Z","shell.execute_reply.started":"2025-10-12T21:45:11.360247Z","shell.execute_reply":"2025-10-12T21:46:01.819767Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def advanced_feature_engineering(df):\n    df = df.copy()\n    \n    # Text length features\n    df['item_name_length'] = df['item_name'].str.len().fillna(0)\n    df['bullet_points_length'] = df['bullet_points'].str.len().fillna(0)\n    df['word_count_item'] = df['item_name'].str.split().str.len().fillna(0)\n    df['word_count_bullet'] = df['bullet_points'].str.split().str.len().fillna(0)\n    \n    # Character-level features\n    df['avg_word_length_item'] = df['item_name_length'] / np.maximum(df['word_count_item'], 1)\n    df['avg_word_length_bullet'] = df['bullet_points_length'] / np.maximum(df['word_count_bullet'], 1)\n    \n    # Brand features (using target encoding if we had full data)\n    brand_counts = df['brand_name'].value_counts()\n    df['brand_frequency'] = df['brand_name'].map(brand_counts).fillna(1)\n    df['is_popular_brand'] = (df['brand_frequency'] > 100).astype(int)\n    \n    # Unit encoding\n    unit_mapping = {\n        'Ounce': 1, 'Count': 2, 'Gram': 3, 'Fluid Ounce': 4, \n        'Pound': 5, 'Milliliter': 6, 'Kit': 7, 'Piece': 8\n    }\n    df['unit_encoded'] = df['unit'].map(unit_mapping).fillna(0)\n    \n    # Value features\n    df['value_log'] = np.log1p(df['value'])\n    df['value_binned'] = pd.cut(df['value'], bins=10, labels=False).fillna(0)\n    \n    # Interaction features\n    df['name_bullet_ratio'] = df['item_name_length'] / np.maximum(df['bullet_points_length'], 1)\n    \n    return df\n\nprint(\"Applying advanced feature engineering...\")\ndf_train = advanced_feature_engineering(df_train)\ndf_test = advanced_feature_engineering(df_test)\n\nprint(f\"New train shape: {df_train.shape}\")\nprint(f\"New test shape: {df_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:01.822721Z","iopub.execute_input":"2025-10-12T21:46:01.823036Z","iopub.status.idle":"2025-10-12T21:46:04.139665Z","shell.execute_reply.started":"2025-10-12T21:46:01.823018Z","shell.execute_reply":"2025-10-12T21:46:04.138977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_advanced(text):\n    if pd.isna(text):\n        return \"\"\n    # Remove special characters but keep important ones\n    # text = re.sub(r'[^\\w\\s\\+-\\*\\/]', '', str(text))\n    # Convert to lowercase\n    text = text.lower()\n    # Remove extra whitespace\n    text = ' '.join(text.split())\n    return text\n\nprint(\"Applying advanced text preprocessing...\")\n\n# Apply preprocessing\nfor df in [df_train, df_test]:\n    df['item_name_clean'] = df['item_name'].apply(preprocess_advanced)\n    df['bullet_points_clean'] = df['bullet_points'].apply(preprocess_advanced)\n    df['brand_name_clean'] = df['brand_name'].apply(preprocess_advanced)\n\nprint(\"Text preprocessing completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:04.140399Z","iopub.execute_input":"2025-10-12T21:46:04.140585Z","iopub.status.idle":"2025-10-12T21:46:05.033459Z","shell.execute_reply.started":"2025-10-12T21:46:04.140571Z","shell.execute_reply":"2025-10-12T21:46:05.032598Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def handle_missing_values(df_train, df_test):\n    # Fill missing values\n    for df in [df_train, df_test]:\n        df['item_name_clean'] = df['item_name_clean'].fillna(\"no_name\")\n        df['bullet_points_clean'] = df['bullet_points_clean'].fillna(\"no_bullets\")\n        df['brand_name_clean'] = df['brand_name_clean'].fillna(\"unknown_brand\")\n        df['value'] = df['value'].fillna(df_train['value'].median())\n        \n        # Fill engineered features\n        numeric_cols = df.select_dtypes(include=[np.number]).columns\n        for col in numeric_cols:\n            if df[col].isnull().any():\n                df[col] = df[col].fillna(df_train[col].median())\n    \n    return df_train, df_test\n\ndf_train, df_test = handle_missing_values(df_train, df_test)\nprint(\"Missing values handled!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:05.034275Z","iopub.execute_input":"2025-10-12T21:46:05.034517Z","iopub.status.idle":"2025-10-12T21:46:07.090016Z","shell.execute_reply.started":"2025-10-12T21:46:05.034489Z","shell.execute_reply":"2025-10-12T21:46:07.08909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Define feature columns\nengineered_features = [\n    'item_name_length', 'bullet_points_length', 'word_count_item', \n    'word_count_bullet', 'avg_word_length_item', 'avg_word_length_bullet',\n    'brand_frequency', 'is_popular_brand', 'unit_encoded', 'value_log',\n    'value_binned', 'name_bullet_ratio'\n]\n\n# Split the data\nX_temp = df_train.drop(['price'], axis=1)\ny_temp = df_train['price']\n\nX_train, X_val, y_train, y_val = train_test_split(\n    X_temp, y_temp, \n    test_size=0.1, \n    random_state=42,\n    stratify=pd.cut(y_temp, bins=10, labels=False)  # Stratified split\n)\n\nprint(f\"Training set: {X_train.shape[0]} samples\")\nprint(f\"Validation set: {X_val.shape[0]} samples\")\nprint(f\"Test set: {df_test.shape[0]} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:07.090888Z","iopub.execute_input":"2025-10-12T21:46:07.091305Z","iopub.status.idle":"2025-10-12T21:46:07.717801Z","shell.execute_reply.started":"2025-10-12T21:46:07.091278Z","shell.execute_reply":"2025-10-12T21:46:07.717141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Identify image embedding columns\nembedding_cols = [col for col in X_train.columns if col.startswith('dim_')]\nprint(f\"Found {len(embedding_cols)} embedding columns\")\n\n# Extract embeddings\nX_train_embeddings = X_train[embedding_cols].values\nX_val_embeddings = X_val[embedding_cols].values\nX_test_embeddings = df_test[embedding_cols].values\n\nprint(f\"Original embeddings shape - Train: {X_train_embeddings.shape}, Val: {X_val_embeddings.shape}\")\n\n# Reduce dimensionality of embeddings\nsvd_embed = TruncatedSVD(n_components=min(500, len(embedding_cols)), random_state=42)\nX_train_embeddings_reduced = svd_embed.fit_transform(X_train_embeddings)\nX_val_embeddings_reduced = svd_embed.transform(X_val_embeddings)\nX_test_embeddings_reduced = svd_embed.transform(X_test_embeddings)\n\nprint(f\"Reduced embeddings shape - Train: {X_train_embeddings_reduced.shape}\")\nprint(f\"Explained variance: {svd_embed.explained_variance_ratio_.sum():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:07.718565Z","iopub.execute_input":"2025-10-12T21:46:07.718748Z","iopub.status.idle":"2025-10-12T21:46:24.037958Z","shell.execute_reply.started":"2025-10-12T21:46:07.718734Z","shell.execute_reply":"2025-10-12T21:46:24.036969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Enhanced TF-IDF with character n-grams\ntfidf_item = TfidfVectorizer(\n    stop_words='english', \n    ngram_range=(1, 3),\n    min_df=2,\n    max_features=8000,\n    analyzer='char_wb',  # Character n-grams\n    max_df=0.8,\n    sublinear_tf=True\n)\n\ntfidf_bullet = TfidfVectorizer(\n    stop_words='english',\n    ngram_range=(1, 2),\n    min_df=3,\n    max_features=5000,\n    max_df=0.85,\n    sublinear_tf=True\n)\n\nprint(\"Fitting enhanced TF-IDF...\")\n\n# Transform text data\nX_train_item_tfidf = tfidf_item.fit_transform(X_train['item_name_clean'])\nX_train_bullet_tfidf = tfidf_bullet.fit_transform(X_train['bullet_points_clean'])\n\nX_val_item_tfidf = tfidf_item.transform(X_val['item_name_clean'])\nX_val_bullet_tfidf = tfidf_bullet.transform(X_val['bullet_points_clean'])\n\nX_test_item_tfidf = tfidf_item.transform(df_test['item_name_clean'])\nX_test_bullet_tfidf = tfidf_bullet.transform(df_test['bullet_points_clean'])\n\nprint(f\"TF-IDF shapes - Item: {X_train_item_tfidf.shape}, Bullet: {X_train_bullet_tfidf.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:24.038813Z","iopub.execute_input":"2025-10-12T21:46:24.039077Z","iopub.status.idle":"2025-10-12T21:46:51.061233Z","shell.execute_reply.started":"2025-10-12T21:46:24.03906Z","shell.execute_reply":"2025-10-12T21:46:51.06033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reduce TF-IDF dimensions\nsvd_item = TruncatedSVD(n_components=800, random_state=42)\nsvd_bullet = TruncatedSVD(n_components=600, random_state=42)\n\nprint(\"Applying dimensionality reduction...\")\n\nX_train_item_reduced = svd_item.fit_transform(X_train_item_tfidf)\nX_train_bullet_reduced = svd_bullet.fit_transform(X_train_bullet_tfidf)\n\nX_val_item_reduced = svd_item.transform(X_val_item_tfidf)\nX_val_bullet_reduced = svd_bullet.transform(X_val_bullet_tfidf)\n\nX_test_item_reduced = svd_item.transform(X_test_item_tfidf)\nX_test_bullet_reduced = svd_bullet.transform(X_test_bullet_tfidf)\n\nprint(f\"Reduced TF-IDF - Item: {X_train_item_reduced.shape}, Bullet: {X_train_bullet_reduced.shape}\")\nprint(f\"Explained variance - Item: {svd_item.explained_variance_ratio_.sum():.4f}, \"\n      f\"Bullet: {svd_bullet.explained_variance_ratio_.sum():.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:46:51.063264Z","iopub.execute_input":"2025-10-12T21:46:51.063604Z","iopub.status.idle":"2025-10-12T21:48:13.462614Z","shell.execute_reply.started":"2025-10-12T21:46:51.063574Z","shell.execute_reply":"2025-10-12T21:48:13.461809Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract engineered features\nX_train_engineered = X_train[engineered_features].values\nX_val_engineered = X_val[engineered_features].values\nX_test_engineered = df_test[engineered_features].values\n\n# Extract numerical features\nX_train_numerical = X_train[['value']].values\nX_val_numerical = X_val[['value']].values\nX_test_numerical = df_test[['value']].values\n\nprint(\"Combining all features...\")\n\n# Combine all features\nX_train_combined = np.hstack([\n    X_train_item_reduced,\n    X_train_bullet_reduced,\n    X_train_numerical,\n    X_train_engineered,\n    X_train_embeddings_reduced\n])\n\nX_val_combined = np.hstack([\n    X_val_item_reduced,\n    X_val_bullet_reduced,\n    X_val_numerical,\n    X_val_engineered,\n    X_val_embeddings_reduced\n])\n\nX_test_combined = np.hstack([\n    X_test_item_reduced,\n    X_test_bullet_reduced,\n    X_test_numerical,\n    X_test_engineered,\n    X_test_embeddings_reduced\n])\n\nprint(f\"Final feature shapes:\")\nprint(f\"Train: {X_train_combined.shape}\")\nprint(f\"Val: {X_val_combined.shape}\")\nprint(f\"Test: {X_test_combined.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:48:13.463469Z","iopub.execute_input":"2025-10-12T21:48:13.463751Z","iopub.status.idle":"2025-10-12T21:48:14.183308Z","shell.execute_reply.started":"2025-10-12T21:48:13.46372Z","shell.execute_reply":"2025-10-12T21:48:14.182586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply target transformation for skewed distributions\nprint(\"Original target statistics:\")\nprint(f\"Skewness: {y_train.skew():.4f}\")\n\n# Choose transformation based on skewness\nif y_train.skew() > 1.0:\n    # Use quantile transformation for highly skewed data\n    qt = QuantileTransformer(output_distribution='normal', random_state=42)\n    y_train_transformed = qt.fit_transform(y_train.values.reshape(-1, 1)).ravel()\n    y_val_transformed = qt.transform(y_val.values.reshape(-1, 1)).ravel()\n    use_transformation = True\n    print(\"Using Quantile Transformation\")\nelse:\n    # Use log transformation for moderately skewed data\n    y_train_transformed = np.log1p(y_train)\n    y_val_transformed = np.log1p(y_val)\n    use_transformation = True\n    print(\"Using Log Transformation\")\n\nprint(\"Target transformation applied!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:48:14.184137Z","iopub.execute_input":"2025-10-12T21:48:14.18441Z","iopub.status.idle":"2025-10-12T21:48:14.210327Z","shell.execute_reply.started":"2025-10-12T21:48:14.184383Z","shell.execute_reply":"2025-10-12T21:48:14.209685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nprint(\"Building ensemble model...\")\n\n# Define base models with optimized parameters\nlgbm = LGBMRegressor(\n    n_estimators=100,\n    learning_rate=0.05,\n    max_depth=8,\n    num_leaves=127,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=0.1,\n    random_state=42,\n    n_jobs=-1\n)\n\n# xgb = XGBRegressor(\n#     n_estimators=2000,\n#     learning_rate=0.05,\n#     max_depth=6,\n#     subsample=0.8,\n#     colsample_bytree=0.8,\n#     reg_alpha=0.1,\n#     reg_lambda=0.1,\n#     random_state=42,\n#     n_jobs=-1\n# )\n\n# catboost = CatBoostRegressor(\n#     iterations=2000,\n#     learning_rate=0.05,\n#     depth=6,\n#     l2_leaf_reg=3,\n#     random_state=42,\n#     verbose=0,\n#     thread_count=-1\n# )\n\n# Stacking Ensemble\nestimators = [\n    ('lgbm', lgbm)\n    # ('xgb', xgb),\n    # ('catboost', catboost)\n]\n\nmodel = StackingRegressor(\n    estimators=estimators,\n    final_estimator=Ridge(alpha=0.1),\n    cv=3,\n    n_jobs=-1\n)\n\nprint(\"Ensemble model created!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:48:14.211019Z","iopub.execute_input":"2025-10-12T21:48:14.211201Z","iopub.status.idle":"2025-10-12T21:48:14.217Z","shell.execute_reply.started":"2025-10-12T21:48:14.211186Z","shell.execute_reply":"2025-10-12T21:48:14.216219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training model with early stopping...\")\n\n# For models that support early stopping\nif use_transformation:\n    # Train on transformed target\n    if hasattr(model, 'fit'):\n        model.fit(X_train_combined, y_train_transformed)\n    else:\n        # Handle individual models with early stopping\n        for name, est in model.named_estimators.items():\n            if name == 'lgbm':\n                est.fit(\n                    X_train_combined, y_train_transformed,\n                    eval_set=[(X_val_combined, y_val_transformed)],\n                    early_stopping_rounds=100,\n                    verbose=100\n                )\n            else:\n                est.fit(X_train_combined, y_train_transformed)\nelse:\n    model.fit(X_train_combined, y_train)\n\nprint(\"Model training completed!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:48:14.217598Z","iopub.execute_input":"2025-10-12T21:48:14.21783Z","iopub.status.idle":"2025-10-12T21:52:12.702963Z","shell.execute_reply.started":"2025-10-12T21:48:14.217815Z","shell.execute_reply":"2025-10-12T21:52:12.702091Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions\nif use_transformation:\n    y_val_pred_transformed = model.predict(X_val_combined)\n    # Inverse transform\n    if 'qt' in locals():\n        y_val_pred = qt.inverse_transform(y_val_pred_transformed.reshape(-1, 1)).ravel()\n    else:\n        y_val_pred = np.expm1(y_val_pred_transformed)\nelse:\n    y_val_pred = model.predict(X_val_combined)\n\n# Calculate metrics\nmse = mean_squared_error(y_val, y_val_pred)\nmae = mean_absolute_error(y_val, y_val_pred)\nr2 = r2_score(y_val, y_val_pred)\n\nprint(\"=\" * 50)\nprint(\"VALIDATION RESULTS\")\nprint(\"=\" * 50)\nprint(f\"MSE: {mse:.4f}\")\nprint(f\"RMSE: {np.sqrt(mse):.4f}\")\nprint(f\"MAE: {mae:.4f}\")\nprint(f\"R¬≤ Score: {r2:.4f}\")\n\n# Additional metrics\nmape = np.mean(np.abs((y_val - y_val_pred) / np.maximum(y_val, 1))) * 100\nprint(f\"MAPE: {mape:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:52:34.850955Z","iopub.execute_input":"2025-10-12T21:52:34.851234Z","iopub.status.idle":"2025-10-12T21:52:34.900943Z","shell.execute_reply.started":"2025-10-12T21:52:34.851218Z","shell.execute_reply":"2025-10-12T21:52:34.900256Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Perform cross-validation for more robust evaluation\nfrom sklearn.model_selection import cross_val_score\n\nprint(\"\\nPerforming cross-validation...\")\ncv_scores = cross_val_score(\n    lgbm, X_train_combined, y_train_transformed if use_transformation else y_train,\n    cv=5, scoring='r2', n_jobs=-1\n)\n\nprint(f\"Cross-validation R¬≤ scores: {cv_scores}\")\nprint(f\"Mean CV R¬≤: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T21:52:43.20457Z","iopub.execute_input":"2025-10-12T21:52:43.205252Z","iopub.status.idle":"2025-10-12T21:59:54.563116Z","shell.execute_reply.started":"2025-10-12T21:52:43.205227Z","shell.execute_reply":"2025-10-12T21:59:54.561955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create detailed validation results\nval_results = pd.DataFrame({\n    'sample_id': X_val['sample_id'],\n    'actual_price': y_val,\n    'predicted_price': y_val_pred,\n    'error': y_val - y_val_pred,\n    'abs_error': np.abs(y_val - y_val_pred)\n})\n\n# Calculate error statistics\nval_results['error_pct'] = (val_results['error'] / np.maximum(val_results['actual_price'], 1)) * 100\nval_results['abs_error_pct'] = np.abs(val_results['error_pct'])\n\n# Save validation results\nval_results.to_csv('validation_predictions_detailed.csv', index=False)\nprint(\"Validation predictions saved!\")\nprint(f\"Top 5 predictions:\")\nprint(val_results.head())\n\nprint(f\"\\nError statistics:\")\nprint(f\"Mean Absolute % Error: {val_results['abs_error_pct'].mean():.2f}%\")\nprint(f\"Median Absolute % Error: {val_results['abs_error_pct'].median():.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T22:00:00.502695Z","iopub.execute_input":"2025-10-12T22:00:00.503365Z","iopub.status.idle":"2025-10-12T22:00:00.568759Z","shell.execute_reply.started":"2025-10-12T22:00:00.503339Z","shell.execute_reply":"2025-10-12T22:00:00.568172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save all preprocessing objects and model\njoblib.dump(model, 'ensemble_model.pkl')\njoblib.dump(tfidf_item, 'tfidf_item_vectorizer.pkl')\njoblib.dump(tfidf_bullet, 'tfidf_bullet_vectorizer.pkl')\njoblib.dump(svd_item, 'svd_item_reducer.pkl')\njoblib.dump(svd_bullet, 'svd_bullet_reducer.pkl')\njoblib.dump(svd_embed, 'svd_embed_reducer.pkl')\n\nif use_transformation:\n    if 'qt' in locals():\n        joblib.dump(qt, 'target_transformer.pkl')\n\nprint(\"Model and preprocessing objects saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T22:00:17.88107Z","iopub.execute_input":"2025-10-12T22:00:17.881657Z","iopub.status.idle":"2025-10-12T22:00:20.104439Z","shell.execute_reply.started":"2025-10-12T22:00:17.881631Z","shell.execute_reply":"2025-10-12T22:00:20.103553Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Making final test predictions...\")\n\nif use_transformation:\n    y_test_pred_transformed = model.predict(X_test_combined)\n    if 'qt' in locals():\n        y_test_pred = qt.inverse_transform(y_test_pred_transformed.reshape(-1, 1)).ravel()\n    else:\n        y_test_pred = np.expm1(y_test_pred_transformed)\nelse:\n    y_test_pred = model.predict(X_test_combined)\n\n# Create submission file\nsubmission_df = pd.DataFrame({\n    'sample_id': df_test['sample_id'],\n    'pred_price': y_test_pred\n})\n\n# Ensure no negative prices\nsubmission_df['pred_price'] = submission_df['pred_price'].clip(lower=0.01)\n\n# Save submission\nsubmission_df.to_csv('submission_ensemble_final.csv', index=False)\nprint(\"Test predictions saved!\")\nprint(f\"Submission stats - Min: {submission_df['pred_price'].min():.2f}, \"\n      f\"Max: {submission_df['pred_price'].max():.2f}, \"\n      f\"Mean: {submission_df['pred_price'].mean():.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-12T22:01:50.660247Z","iopub.execute_input":"2025-10-12T22:01:50.660948Z","iopub.status.idle":"2025-10-12T22:01:51.252488Z","shell.execute_reply.started":"2025-10-12T22:01:50.660897Z","shell.execute_reply":"2025-10-12T22:01:51.251628Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"=\" * 60)\nprint(\"FINAL TRAINING SUMMARY\")\nprint(\"=\" * 60)\nprint(f\"Training samples: {X_train.shape[0]}\")\nprint(f\"Validation samples: {X_val.shape[0]}\")\nprint(f\"Test samples: {df_test.shape[0]}\")\nprint(f\"Final feature dimensions: {X_train_combined.shape[1]}\")\nprint(f\"Validation R¬≤ Score: {r2:.4f}\")\nprint(f\"Validation MAE: {mae:.4f}\")\nprint(f\"Cross-validation R¬≤: {cv_scores.mean():.4f}\")\nprint(f\"Target transformation: {'Yes' if use_transformation else 'No'}\")\nprint(f\"Model type: Stacking Ensemble\")\n\nprint(\"\\nüìÅ Files created:\")\nprint(\"- ensemble_model.pkl\")\nprint(\"- tfidf_item_vectorizer.pkl\")\nprint(\"- tfidf_bullet_vectorizer.pkl\")\nprint(\"- svd_*_reducer.pkl\")\nprint(\"- validation_predictions_detailed.csv\")\nprint(\"- submission_ensemble_final.csv\")\n\nprint(\"\\nüéØ Expected improvements:\")\nprint(\"- Feature engineering: +0.05-0.10 R¬≤\")\nprint(\"- Advanced text processing: +0.02-0.04 R¬≤\") \nprint(\"- Ensemble modeling: +0.03-0.06 R¬≤\")\nprint(\"- Target transformation: +0.02-0.04 R¬≤\")\nprint(f\"Total expected R¬≤: 0.60 - 0.75\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}